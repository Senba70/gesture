{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fd9de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.11.0\n",
      "Torchvision Version:  0.12.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca50b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./../Picture_resize\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"squeezenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2346de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d38a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4360417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(fc): nn.Linear(in_features=512, out_features=1000, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e92051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fc = nn.Linear(512, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3711452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde58a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "176c42df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d7078fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a0ec819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\H3rb\\anaconda3\\envs\\NeRF\\lib\\site-packages\\torch\\nn\\functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.8313 Acc: 0.4362\n",
      "val Loss: 0.7105 Acc: 0.5581\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.6429 Acc: 0.6064\n",
      "val Loss: 0.6614 Acc: 0.5581\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.4780 Acc: 0.7979\n",
      "val Loss: 0.7243 Acc: 0.5349\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.3684 Acc: 0.8723\n",
      "val Loss: 0.7860 Acc: 0.5116\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.3837 Acc: 0.8298\n",
      "val Loss: 1.0067 Acc: 0.5116\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.2133 Acc: 0.9043\n",
      "val Loss: 0.6576 Acc: 0.6279\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.1865 Acc: 0.9255\n",
      "val Loss: 0.6335 Acc: 0.6744\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.1485 Acc: 0.9468\n",
      "val Loss: 0.9296 Acc: 0.6512\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.1536 Acc: 0.9149\n",
      "val Loss: 0.6533 Acc: 0.7674\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.0817 Acc: 0.9894\n",
      "val Loss: 0.5188 Acc: 0.6977\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.1305 Acc: 0.9574\n",
      "val Loss: 0.6545 Acc: 0.7674\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.0894 Acc: 0.9787\n",
      "val Loss: 0.6924 Acc: 0.7674\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.1085 Acc: 0.9681\n",
      "val Loss: 0.5257 Acc: 0.6977\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.0772 Acc: 0.9787\n",
      "val Loss: 0.5605 Acc: 0.7907\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.0907 Acc: 0.9681\n",
      "val Loss: 0.5346 Acc: 0.6512\n",
      "\n",
      "Training complete in 0m 39s\n",
      "Best val Acc: 0.790698\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa5d67db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.7226 Acc: 0.5532\n",
      "val Loss: 0.6929 Acc: 0.5581\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.6948 Acc: 0.4574\n",
      "val Loss: 0.6933 Acc: 0.4419\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.6939 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.6933 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.6934 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.6932 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.6930 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.6932 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.6932 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.6930 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.6930 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.6932 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.6934 Acc: 0.4574\n",
      "val Loss: 0.6931 Acc: 0.4419\n",
      "\n",
      "Training complete in 0m 38s\n",
      "Best val Acc: 0.558140\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the non-pretrained version of the model used for this run\n",
    "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "\n",
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f055fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "test_dir = \"./../Picture_resize\"\n",
    "test = glob.glob('./../Picture_resize/val/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d84c55b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(class_names[class_id])\n\u001b[0;32m     20\u001b[0m imgs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_dir \u001b[38;5;129;01min\u001b[39;00m (\u001b[43mtest_dir\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m)\u001b[38;5;241m.\u001b[39miterdir():\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(class_dir\u001b[38;5;241m.\u001b[39miterdir())[:\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m     23\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "def show_prediction(model, transform, imgs, n=3):\n",
    "    for img in imgs:\n",
    "        # 1. PIL Image を標準化したテンソルにする。\n",
    "        # 2. バッチ次元を追加する。 (C, H, W) -> (1, C, H, W)\n",
    "        # 3. 計算するデバイスに転送する。\n",
    "        inputs = transform(img).unsqueeze(dim=0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 順伝搬を行う。\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # 確率の最も高いクラスを予測ラベルとする。\n",
    "            class_id = int(outputs.argmax(dim=1)[0])\n",
    "\n",
    "        # 推論結果を表示する。\n",
    "        display.display(img.resize((224, 224)))\n",
    "        print(class_names[class_id])\n",
    "\n",
    "\n",
    "imgs = []\n",
    "for class_dir in (test_dir/ \"val\").iterdir():\n",
    "    for img_path in sorted(class_dir.iterdir())[:2]:\n",
    "        img = Image.open(img_path)\n",
    "        imgs.append(img)\n",
    "\n",
    "show_prediction(model_ft, data_transforms[\"val\"], imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe3f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
